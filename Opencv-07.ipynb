{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa8f32a-56cf-47e8-9584-156783802573",
   "metadata": {},
   "source": [
    "# üì∏ Webcam Image Capture using OpenCV\n",
    "\n",
    "This Python script captures a single image from the default webcam and displays it in a window using the OpenCV library.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <a href=\"https://colab.research.google.com/github/MinooSdpr/Machine-Learning-101/blob/main/Session%2017/17_3%20-%20Keras%20Project%20Exercise.ipynb\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" />\n",
    "  </a>\n",
    "  &nbsp;\n",
    "  <a href=\"https://github.com/MinooSdpr/Machine-Learning-101/blob/main/Session%2017/17_3%20-%20Keras%20Project%20Exercise.ipynb\">\n",
    "    <img src=\"https://img.shields.io/badge/Open%20in-GitHub-24292e?logo=github&logoColor=white\" alt=\"Open In GitHub\" />\n",
    "  </a>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ Camera Index\n",
    "\n",
    "* `0` is usually the **default camera** (e.g., the built-in webcam).\n",
    "* If you have multiple cameras connected:\n",
    "\n",
    "  * `1` would be the second camera,\n",
    "  * `2` for the third, and so on.\n",
    "* You can also pass a **video file path or URL** (e.g., `'video.mp4'` or an IP camera stream).\n",
    "\n",
    "### üì¶ What is `cam`?\n",
    "\n",
    "* `cam` is a `cv2.VideoCapture` object that provides methods to:\n",
    "\n",
    "  * Read frames from the camera (`.read()`)\n",
    "  * Check if the camera opened successfully (`.isOpened()`)\n",
    "  * Set properties like resolution or frame rate (`.set()`)\n",
    "  * Release the camera resource when done (`.release()`)\n",
    "\n",
    "#### ‚ö†Ô∏è Notes\n",
    "\n",
    "* Make sure your system has a webcam connected and accessible.\n",
    "* This script captures only **one frame**. To capture video or multiple frames, you‚Äôd need a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9ddcb0-1f31-40f5-b58a-e2cebccbb3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "cam = cv.VideoCapture(0)\n",
    "\n",
    "ret, img = cam.read()\n",
    "cv.imshow('camera pic',img)\n",
    "print(ret)\n",
    "cv.waitKey(0)\n",
    "cam.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5480f-acd2-418b-8882-4d8b6e954db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, img = cam.read()\n",
    "    cv.imshow('camera pic',img)\n",
    "    if cv.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f59c01-5626-4bb0-8369-6450cf6880ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c10676-83a0-4bf3-9420-7c17eea7ae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.imshow('camera pic',img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef221903-2f42-4390-841e-e99fb406dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.imwrite('camera.png',img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f11a8b-c269-410f-9e8b-58b233568f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv.VideoCapture(0)\n",
    "\n",
    "print(cam.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "print(cam.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "i=0\n",
    "\n",
    "while True:\n",
    "    ret, img = cam.read()\n",
    "    cv.imshow('camera pic',img)\n",
    "    key = cv.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    if key == ord('s'):\n",
    "        cv.imwrite(f'camera{i}.png',img)\n",
    "        i+=1\n",
    "cam.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440bdf9b-fed7-43fc-93bc-3b267a44daae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "cam = cv.VideoCapture(0)\n",
    "now = datetime.datetime.now()\n",
    "i=0\n",
    "if cam.isOpened():\n",
    "    while True:\n",
    "        ret, img = cam.read()\n",
    "        cv.imshow('camera pic',img)\n",
    "        \n",
    "        key = cv.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        if key == ord('s'):\n",
    "            cv.imwrite(f'camera {now} {i}.png',img)\n",
    "            i+=1\n",
    "else:\n",
    "    print('Something wrong in connecting with camera!')\n",
    "cam.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991cb367-de1d-44fa-b84b-e0bc07551128",
   "metadata": {},
   "source": [
    "**inRange** OpenCV function that checks if elements in an array (image) lie between two specified values. It returns a binary mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203992d-3d5c-40a7-be7a-69d5ed5acf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    rec, frame = cap.read()\n",
    "    frame_hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n",
    "    \n",
    "    # This particular range targets a shade of red\n",
    "    lower_red = np.array([100,50,50])\n",
    "    upper_red = np.array([116,255,255]) \n",
    "    \n",
    "\n",
    "    mask_red = cv.inRange(frame_hsv, lower_red, upper_red)\n",
    "    frame_masked = cv.bitwise_and(frame, frame, mask = mask_red)\n",
    "\n",
    "    cv.imshow('frame', frame)\n",
    "    cv.imshow('mask_red', mask_red)\n",
    "    cv.imshow('frame_masked', frame_masked)\n",
    "    keyexit = cv.waitKey(5) & 0xFF\n",
    "    if keyexit == 27:\n",
    "        break\n",
    "\n",
    "cv.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd27af2-ee56-4254-80bb-d4ea7a199f08",
   "metadata": {},
   "source": [
    "## üòÑ Real-Time Face, Eye, and Smile Detection with OpenCV\n",
    "\n",
    "\n",
    "### Haar Cascade Classifier\n",
    "- A machine learning-based approach where a cascade function is trained from positive and negative images.\n",
    "- OpenCV provides XML files for commonly used objects (face, eye, smile, etc.).\n",
    "[link](https://github.com/opencv/opencv/tree/4.x/data/haarcascades)\n",
    "---\n",
    "\n",
    "### üîç How Detection Works\n",
    "\n",
    "1. Convert the video frame to **grayscale** (Haar cascades require grayscale input).\n",
    "2. Detect faces in the frame.\n",
    "3. For each detected face:\n",
    "   * Detect eyes and smiles **within the face region only** (ROI = Region of Interest).\n",
    "4. Draw rectangles on the detected features.\n",
    "5. Display the annotated video feed in real time.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Parameters Explained\n",
    "\n",
    "| Parameter      | Description                                                                  |\n",
    "| -------------- | ---------------------------------------------------------------------------- |\n",
    "| `scaleFactor`  | Specifies how much the image size is reduced at each scale (e.g., 1.3 = 30%) |\n",
    "| `minNeighbors` | How many neighbors each rectangle should have to retain it                   |\n",
    "| `roi_gray`     | Grayscale image inside detected face region                                  |\n",
    "| `roi_color`    | Original color image inside detected face region                             |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24671157-8c3a-4053-8b23-a27ada3c400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "face_cascade = cv.CascadeClassifier(cv.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv.CascadeClassifier(cv.data.haarcascades + 'haarcascade_eye.xml')\n",
    "smile_cascade = cv.CascadeClassifier(cv.data.haarcascades + 'haarcascade_smile.xml')\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 255), 2)\n",
    "\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray, scaleFactor=1.1, minNeighbors=10)\n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "            cv.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
    "\n",
    "        smiles = smile_cascade.detectMultiScale(roi_gray, scaleFactor=1.7, minNeighbors=22)\n",
    "        for (sx, sy, sw, sh) in smiles:\n",
    "            cv.rectangle(roi_color, (sx, sy), (sx + sw, sy + sh), (0, 0, 255), 2)\n",
    "\n",
    "    cv.imshow('Face, Eye & Smile Detection', frame)\n",
    "\n",
    "    if cv.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d5ea6-e397-4013-b99a-0f86b1fbe768",
   "metadata": {},
   "source": [
    "## üñêÔ∏è Real-Time Hand Detection and Distance Measurement with cvzone\n",
    "\n",
    "This notebook demonstrates how to use **cvzone's `HandTrackingModule`** to detect hands and measure distances between fingers using a webcam.\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ Requirements\n",
    "\n",
    "- `cv2` (OpenCV): For video capture and display\n",
    "- `cvzone`: High-level wrapper around OpenCV and MediaPipe\n",
    "- A working webcam\n",
    "\n",
    "Install with:\n",
    "```bash\n",
    "pip install cvzone\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Core Modules Used\n",
    "\n",
    "### üìÅ `cvzone.HandTrackingModule.HandDetector`\n",
    "\n",
    "This is a wrapper around **MediaPipe Hands**. It simplifies hand landmark detection and interaction, returning hand landmarks and providing helper functions like distance calculation.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Parameter Explanation\n",
    "\n",
    "| Parameter      | Description                                                     |\n",
    "| -------------- | --------------------------------------------------------------- |\n",
    "| `detectionCon` | Minimum detection confidence (0 to 1). `0.5` is a good default. |\n",
    "| `maxHands`     | Maximum number of hands to detect at once.                      |\n",
    "\n",
    "---\n",
    "\n",
    "### üìê Distance Measurement\n",
    "\n",
    "The function:\n",
    "\n",
    "```python\n",
    "length, info, frame = detector.findDistance(point1, point2, frame)\n",
    "```\n",
    "\n",
    "* Calculates **Euclidean distance** between two points.\n",
    "* Draws a **line and midpoint** between them on the frame.\n",
    "* Returns:\n",
    "\n",
    "  * `length`: Distance in pixels\n",
    "  * `info`: (x1, y1, x2, y2, cx, cy)\n",
    "  * `frame`: Updated frame with visuals\n",
    "\n",
    "\n",
    "## üöÄ Use Cases\n",
    "\n",
    "* Gesture-controlled interfaces\n",
    "* Virtual ruler (measuring finger distance)\n",
    "* Hand sign recognition\n",
    "* Touchless UI controls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e48c00-a9de-4332-9e90-f63b99e6ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvzone.HandTrackingModule import HandDetector\n",
    "\n",
    "cap = cv.VideoCapture(1)\n",
    "detector = HandDetector(detectionCon=0.5, maxHands=2)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    hands, frame = detector.findHands(frame)\n",
    "    if hands:\n",
    "        hand1 = hands[0]\n",
    "        lmList1 = hand1[\"lmList\"]  # List of 21 landmarks for the hand\n",
    "\n",
    "        point1 = lmList1[4][:2] \n",
    "        point2 = lmList1[8][:2] \n",
    "\n",
    "        length, info, frame = detector.findDistance(point1, point2, frame)\n",
    "\n",
    "    cv.imshow('Hand Detection', frame)\n",
    "    if cv.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3988fa93-dae5-406c-8cfb-0f343d4b3c61",
   "metadata": {},
   "source": [
    "## ü§ñ Real-Time Face Detection and Face Mesh Tracking with cvzone\n",
    "\n",
    "This notebook uses `cvzone` to perform:\n",
    "- üì∏ Face Detection (bounding boxes + center point)\n",
    "- üß† Face Mesh Detection (468 facial landmarks)\n",
    "\n",
    "---\n",
    "\n",
    "### üî≤ `cvzone.FaceDetectionModule.FaceDetector`\n",
    "\n",
    "* Detects **faces** in real-time using a bounding box\n",
    "* Provides:\n",
    "\n",
    "  * Bounding box position and size\n",
    "  * Confidence score\n",
    "  * Center of the detected face\n",
    "\n",
    "### üß¨ `cvzone.FaceMeshModule.FaceMeshDetector`\n",
    "\n",
    "* Tracks **468 facial landmarks** per face (based on MediaPipe Face Mesh)\n",
    "* Used for:\n",
    "\n",
    "  * Expression tracking\n",
    "  * Face morphing\n",
    "  * Eye/mouth detection\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Parameter Details\n",
    "\n",
    "#### FaceDetector\n",
    "\n",
    "| Parameter         | Description                                 |\n",
    "| ----------------- | ------------------------------------------- |\n",
    "| `minDetectionCon` | Minimum confidence for face detection (0-1) |\n",
    "\n",
    "#### FaceMeshDetector\n",
    "\n",
    "| Parameter  | Description                      |\n",
    "| ---------- | -------------------------------- |\n",
    "| `maxFaces` | Maximum number of faces to track |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e04d16-024b-404b-9bd7-618608b4de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvzone.FaceDetectionModule import FaceDetector\n",
    "from cvzone.FaceMeshModule import FaceMeshDetector\n",
    "\n",
    "cap = cv.VideoCapture(1)\n",
    "detector = FaceDetector()\n",
    "meshdetector = FaceMeshDetector(maxFaces=1)\n",
    "\n",
    "while(True):\n",
    "    rec, frame = cap.read()\n",
    "    frame, bbox = detector.findFaces(frame)\n",
    "    frame, faces = meshdetector.findFaceMesh(frame)\n",
    "\n",
    "    if bbox:\n",
    "        center = bbox[0][\"center\"]   \n",
    "        #cv.circle(frame, center, 5, (255, 0, 255), cv.FILLED)\n",
    "    cv.imshow('frame', frame)\n",
    "    keyexit = cv.waitKey(5) & 0xFF\n",
    "    if keyexit == 27:\n",
    "        break\n",
    "\n",
    "cv.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0532f574-9b10-4519-a644-c902e0f202ca",
   "metadata": {},
   "source": [
    "<div style=\"float:right;\">\n",
    "  <a href=\"https://github.com/MinooSdpr/Machine-Learning-101/blob/main/Session%2018/01%20-%20CNN%20.pptx\"\n",
    "     style=\"\n",
    "       display:inline-block;\n",
    "       padding:8px 20px;\n",
    "       background-color:#414f6f;\n",
    "       color:white;\n",
    "       border-radius:12px;\n",
    "       text-decoration:none;\n",
    "       font-family:sans-serif;\n",
    "       transition:background-color 0.3s ease;\n",
    "     \">\n",
    "    ‚ñ∂Ô∏è Next\n",
    "  </a>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
